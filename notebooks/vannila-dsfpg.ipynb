{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aff89142-705c-46c7-8bf5-dae1fb09eb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Flow failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'flow'\n",
      "Warning: CARLA failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'carla'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from deep_rl import *\n",
    "from deep_rl.agent.DSFPG_agent import DSFPGAgent\n",
    "from deep_rl.agent.DSFPG_delayed import DSFPGAgent_td3\n",
    "\n",
    "select_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5731a42c-89f1-4a7c-8de6-44d98afb0da1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yiranwang/anaconda3/envs/DPC/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "2021-07-08 03:52:45,938 - root - INFO: steps 0, 80659692.31 steps/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "==================== Start RL Phase =====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-08 03:52:50,403 - root - INFO: steps 0, episodic_return_test 0.00(0.00)\n",
      "2021-07-08 03:52:50,631 - root - INFO: steps 999, episodic_return_train 0.0\n",
      "2021-07-08 03:52:50,631 - root - INFO: steps 1000, 213.10 steps/s\n",
      "2021-07-08 03:52:50,857 - root - INFO: steps 1999, episodic_return_train 0.0\n",
      "2021-07-08 03:52:50,857 - root - INFO: steps 2000, 4424.32 steps/s\n",
      "2021-07-08 03:52:51,085 - root - INFO: steps 2999, episodic_return_train 0.0\n",
      "2021-07-08 03:52:51,086 - root - INFO: steps 3000, 4391.40 steps/s\n",
      "2021-07-08 03:52:51,313 - root - INFO: steps 3999, episodic_return_train 0.0\n",
      "2021-07-08 03:52:51,313 - root - INFO: steps 4000, 4394.96 steps/s\n",
      "2021-07-08 03:52:51,541 - root - INFO: steps 4999, episodic_return_train 0.0\n",
      "2021-07-08 03:52:51,541 - root - INFO: steps 5000, 4394.34 steps/s\n",
      "2021-07-08 03:52:51,768 - root - INFO: steps 5999, episodic_return_train 0.0\n",
      "2021-07-08 03:52:51,769 - root - INFO: steps 6000, 4402.37 steps/s\n",
      "2021-07-08 03:52:51,996 - root - INFO: steps 6999, episodic_return_train 0.0\n",
      "2021-07-08 03:52:51,997 - root - INFO: steps 7000, 4394.57 steps/s\n",
      "2021-07-08 03:52:52,223 - root - INFO: steps 7999, episodic_return_train 0.0\n",
      "2021-07-08 03:52:52,224 - root - INFO: steps 8000, 4410.09 steps/s\n",
      "2021-07-08 03:52:52,451 - root - INFO: steps 8999, episodic_return_train 0.0\n",
      "2021-07-08 03:52:52,451 - root - INFO: steps 9000, 4404.76 steps/s\n",
      "2021-07-08 03:52:52,678 - root - INFO: steps 9999, episodic_return_train 0.0\n",
      "2021-07-08 03:52:52,699 - root - INFO: steps 10000, 4045.62 steps/s\n",
      "2021-07-08 03:52:56,764 - root - INFO: steps 10000, episodic_return_test 0.11(0.08)\n",
      "2021-07-08 03:53:05,355 - root - INFO: steps 10999, episodic_return_train 0.0\n",
      "2021-07-08 03:53:05,365 - root - INFO: steps 11000, 78.95 steps/s\n",
      "2021-07-08 03:53:13,919 - root - INFO: steps 11999, episodic_return_train 0.0\n",
      "2021-07-08 03:53:13,928 - root - INFO: steps 12000, 116.78 steps/s\n",
      "2021-07-08 03:53:22,483 - root - INFO: steps 12999, episodic_return_train 0.0\n",
      "2021-07-08 03:53:22,496 - root - INFO: steps 13000, 116.73 steps/s\n",
      "2021-07-08 03:53:31,093 - root - INFO: steps 13999, episodic_return_train 0.0\n",
      "2021-07-08 03:53:31,105 - root - INFO: steps 14000, 116.17 steps/s\n",
      "2021-07-08 03:53:39,737 - root - INFO: steps 14999, episodic_return_train 0.0\n",
      "2021-07-08 03:53:39,747 - root - INFO: steps 15000, 115.71 steps/s\n",
      "2021-07-08 03:53:48,339 - root - INFO: steps 15999, episodic_return_train 0.0\n",
      "2021-07-08 03:53:48,348 - root - INFO: steps 16000, 116.28 steps/s\n",
      "2021-07-08 03:53:57,044 - root - INFO: steps 16999, episodic_return_train 0.0\n",
      "2021-07-08 03:53:57,053 - root - INFO: steps 17000, 114.87 steps/s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e1f30793f621>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m# dsfpg = dsfpg_online(game='Hopper-v2', linear=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m \u001b[0mdsfpg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdsfpg_online\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dm-hopper-hop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-e1f30793f621>\u001b[0m in \u001b[0;36mdsfpg_online\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workplace/DSFPG/deep_rl/agent/DSFPG_agent.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# print(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_online\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep_online\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workplace/DSFPG/deep_rl/agent/DSFPG_agent.py\u001b[0m in \u001b[0;36mstep_online\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0msf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0mpolicy_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def dsfpg_online(**kwargs):\n",
    "    generate_tag(kwargs)\n",
    "    kwargs.setdefault('log_level', 0)\n",
    "    '''kwargs.setdefault('n_step', 1)\n",
    "    kwargs.setdefault('replay_cls', UniformReplay)\n",
    "    kwargs.setdefault('async_replay', True)'''\n",
    "    \n",
    "    config = Config()\n",
    "    config.merge(kwargs)\n",
    "\n",
    "    config.offline = False\n",
    "    config.task_fn = lambda: Task(config.game)\n",
    "    config.eval_env = config.task_fn()\n",
    "    config.max_steps = int(1e6)\n",
    "    config.eval_interval = int(1e4)\n",
    "    config.eval_episodes = 10\n",
    "    config.td3_delay = 2\n",
    "    \n",
    "    config.network_fn = lambda: DSFPGNet(\n",
    "        config.state_dim, config.action_dim,\n",
    "        actor_body=FCBody(config.state_dim, (400, 300), gate=F.relu),\n",
    "        # critic_body=FCBody(config.state_dim, (400, 300), gate=F.relu),\n",
    "        critic_body=SF_FCBody(config.state_dim, (400, 300), gate=F.relu, linear=config.linear),\n",
    "        sf_body=FCBody(config.state_dim+config.action_dim, (400, 300), gate=F.relu),\n",
    "        actor_opt_fn=lambda params: torch.optim.Adam(params, lr=1e-3),\n",
    "        critic_opt_fn=lambda params: torch.optim.Adam(params, lr=1e-3),\n",
    "        sf_opt_fn=lambda params: torch.optim.Adam(params, lr=1e-3))\n",
    "\n",
    "    # config.replay_fn = lambda: ReplayWrapper(UniformReplay, replay_kwargs)\n",
    "\n",
    "    config.replay_fn = lambda: UniformReplay(memory_size=int(1e6), batch_size=256)\n",
    "    config.discount = 0.99\n",
    "    config.random_process_fn = lambda: OrnsteinUhlenbeckProcess(\n",
    "        size=(config.action_dim,), std=LinearSchedule(0.2))\n",
    "    ''' config.random_process_fn = lambda: GaussianProcess(\n",
    "        size=(config.action_dim,), std=LinearSchedule(0.1))'''\n",
    "    config.warm_up = int(1e4)\n",
    "    # config.pre_training_steps = int(1e6)\n",
    "    config.target_network_mix = 5e-3\n",
    "    # run_steps(DSFPGAgent(config))\n",
    "    \n",
    "    '''\n",
    "    change the following line to switch between vanilla dsfpg / delayed vanilla dsfpg\n",
    "    '''\n",
    "    agent = DSFPGAgent(config)\n",
    "    # config = agent.config\n",
    "    agent_name = agent.__class__.__name__\n",
    "    t0 = time.time()\n",
    "    agent.pre_training = False\n",
    "    '''while True:\n",
    "        if config.log_interval and not agent.unsupervised_steps % config.log_interval:\n",
    "            agent.logger.info('steps %d, %.2f steps/s' % (agent.unsupervised_steps, config.log_interval / (time.time() - t0)))\n",
    "            t0 = time.time()\n",
    "        if config.max_steps and agent.unsupervised_steps >= config.pre_training_steps:\n",
    "            agent.close()\n",
    "            break\n",
    "        agent.step()\n",
    "        agent.switch_task()'''\n",
    "\n",
    "\n",
    "    print('==================== Start RL Phase =====================')\n",
    "    agent.pre_training = False\n",
    "    while True:\n",
    "        if config.save_interval and not agent.total_steps % config.save_interval:\n",
    "            agent.save('data/%s-%s-%d' % (agent_name, config.tag, agent.total_steps))\n",
    "        if config.log_interval and not agent.total_steps % config.log_interval:\n",
    "            agent.logger.info('steps %d, %.2f steps/s' % (agent.total_steps, config.log_interval / (time.time() - t0)))\n",
    "            t0 = time.time()\n",
    "        if config.eval_interval and not agent.total_steps % config.eval_interval:\n",
    "            agent.eval_episodes()\n",
    "        if config.max_steps and agent.total_steps >= config.max_steps:\n",
    "            return agent\n",
    "            break\n",
    "        agent.step()\n",
    "        agent.switch_task()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# dsfpg = dsfpg_online(game='Hopper-v2', linear=True)\n",
    "dsfpg = dsfpg_online(game='dm-hopper-hop', linear=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DPC",
   "language": "python",
   "name": "dpc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
