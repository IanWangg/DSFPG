{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5779f82-66fd-41a2-8ecb-67640474c81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yiranwang/anaconda3/envs/DPC/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('Hopper-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "690b56fe-d173-4a56-a692-96945bf7d405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.24943720e+00, -2.87405197e-03, -9.19906749e-03, -1.23329904e-03,\n",
       "        -1.87178906e-04, -1.78119501e-01, -6.09777862e-02, -1.66516963e+00,\n",
       "        -1.33327390e+00, -1.13110405e+00, -1.30295591e-01]),\n",
       " 0.9102182663706267,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = env.action_space.sample()\n",
    "env.reset()\n",
    "env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c811688-be44-4d9f-b4bc-5472b66a3fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class WNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_units=(400, 300), gate=F.relu):\n",
    "        super(WNet, self).__init__()\n",
    "        fc_list = []\n",
    "        for i in range(len(hidden_units)):\n",
    "            in_dim = input_dim if i == 0 else hidden_units[i - 1]\n",
    "            out_dim = hidden_units[i]\n",
    "            layer = nn.Linear(in_dim, out_dim)\n",
    "            fc_list.append(layer)\n",
    "            \n",
    "        fc_list.append(nn.Linear(hidden_units[-1], output_dim))\n",
    "            \n",
    "        self.fc_list = nn.ModuleList(fc_list)\n",
    "        self.gate = gate\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        self.to(self.device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x.to(self.device)\n",
    "        for layer in self.fc_list:\n",
    "            x = self.gate(layer(x))\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e081793-b3d7-4029-b916-58bfacb53c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the replay buffer\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from deep_rl import *\n",
    "\n",
    "replay = UniformReplay(memory_size=int(1e6), batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6cebdb1d-6101-458c-ab89-c134e1b4c36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "net = WNet(11, 1)\n",
    "opt = Adam(list(net.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3b70e852-a79a-41a7-a078-255495c22a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 20000,  Loss : 0.0010396852158010006\n",
      "Steps : 30000,  Loss : 0.0026800506748259068\n",
      "Steps : 40000,  Loss : 0.0010459991171956062\n",
      "Steps : 50000,  Loss : 0.002869775053113699\n",
      "Steps : 60000,  Loss : 0.0013931540306657553\n",
      "Steps : 70000,  Loss : 0.0008314452134072781\n",
      "Steps : 80000,  Loss : 0.001108307158574462\n",
      "Steps : 90000,  Loss : 0.001379550900310278\n",
      "Steps : 100000,  Loss : 0.0064567821100354195\n",
      "Steps : 110000,  Loss : 0.004761221818625927\n",
      "Steps : 120000,  Loss : 0.0008333222940564156\n",
      "Steps : 130000,  Loss : 0.0007928359555080533\n",
      "Steps : 140000,  Loss : 0.0010601794347167015\n",
      "Steps : 150000,  Loss : 0.0008834320469759405\n",
      "Steps : 160000,  Loss : 0.002309125615283847\n",
      "Steps : 170000,  Loss : 0.0023637961130589247\n",
      "Steps : 180000,  Loss : 0.002173013286665082\n",
      "Steps : 190000,  Loss : 0.0020743415225297213\n",
      "Steps : 200000,  Loss : 0.0018699580105021596\n",
      "Steps : 210000,  Loss : 0.0017723074415698647\n",
      "Steps : 220000,  Loss : 0.002121815225109458\n",
      "Steps : 230000,  Loss : 0.004485963378101587\n",
      "Steps : 240000,  Loss : 0.0009073533583432436\n",
      "Steps : 250000,  Loss : 0.003955623600631952\n",
      "Steps : 260000,  Loss : 0.0038979765959084034\n",
      "Steps : 270000,  Loss : 0.0010135640623047948\n",
      "Steps : 280000,  Loss : 0.001052853069268167\n",
      "Steps : 290000,  Loss : 0.0009259682847186923\n",
      "Steps : 300000,  Loss : 0.002299731131643057\n",
      "Steps : 310000,  Loss : 0.0008454456692561507\n",
      "Steps : 320000,  Loss : 0.0017181024886667728\n",
      "Steps : 330000,  Loss : 0.004715803079307079\n",
      "Steps : 340000,  Loss : 0.0012691833544522524\n",
      "Steps : 350000,  Loss : 0.0008860068046487868\n",
      "Steps : 360000,  Loss : 0.00208855839446187\n",
      "Steps : 370000,  Loss : 0.001345052500255406\n",
      "Steps : 380000,  Loss : 0.0008091161726042628\n",
      "Steps : 390000,  Loss : 0.0009076269925571978\n",
      "Steps : 400000,  Loss : 0.00325979245826602\n",
      "Steps : 410000,  Loss : 0.002388330176472664\n",
      "Steps : 420000,  Loss : 0.0007860161131247878\n",
      "Steps : 430000,  Loss : 0.0011156946420669556\n",
      "Steps : 440000,  Loss : 0.001074144965969026\n",
      "Steps : 450000,  Loss : 0.003815942443907261\n",
      "Steps : 460000,  Loss : 0.002209451049566269\n",
      "Steps : 470000,  Loss : 0.0029522767290472984\n",
      "Steps : 480000,  Loss : 0.0019512440776452422\n",
      "Steps : 490000,  Loss : 0.001314813969656825\n",
      "Steps : 500000,  Loss : 0.0010694044176489115\n",
      "Steps : 510000,  Loss : 0.0009665732504799962\n",
      "Steps : 520000,  Loss : 0.0009599100449122488\n",
      "Steps : 530000,  Loss : 0.0018707106355577707\n",
      "Steps : 540000,  Loss : 0.0013683000579476357\n",
      "Steps : 550000,  Loss : 0.0010517393238842487\n",
      "Steps : 560000,  Loss : 0.002912109484896064\n",
      "Steps : 570000,  Loss : 0.004775238689035177\n",
      "Steps : 580000,  Loss : 0.003294000867754221\n",
      "Steps : 590000,  Loss : 0.0015281490050256252\n",
      "Steps : 600000,  Loss : 0.0013256743550300598\n",
      "Steps : 610000,  Loss : 0.0016275284579023719\n",
      "Steps : 620000,  Loss : 0.0019882849883288145\n",
      "Steps : 630000,  Loss : 0.0012814397923648357\n",
      "Steps : 640000,  Loss : 0.0009023561142385006\n",
      "Steps : 650000,  Loss : 0.0011866615386679769\n",
      "Steps : 660000,  Loss : 0.0018084578914567828\n",
      "Steps : 670000,  Loss : 0.0008993163937702775\n",
      "Steps : 680000,  Loss : 0.0013282869476824999\n",
      "Steps : 690000,  Loss : 0.0012517344439402223\n",
      "Steps : 700000,  Loss : 0.0012938479194417596\n",
      "Steps : 710000,  Loss : 0.0019280731212347746\n",
      "Steps : 720000,  Loss : 0.000812542624771595\n",
      "Steps : 730000,  Loss : 0.0009709193836897612\n",
      "Steps : 740000,  Loss : 0.0013893786817789078\n",
      "Steps : 750000,  Loss : 0.0010075694881379604\n",
      "Steps : 760000,  Loss : 0.0012758527882397175\n",
      "Steps : 770000,  Loss : 0.0007120074005797505\n",
      "Steps : 780000,  Loss : 0.0021661058999598026\n",
      "Steps : 790000,  Loss : 0.001191703137010336\n",
      "Steps : 800000,  Loss : 0.0017724884673953056\n",
      "Steps : 810000,  Loss : 0.010712598450481892\n",
      "Steps : 820000,  Loss : 0.0008130067144520581\n",
      "Steps : 830000,  Loss : 0.0025526888202875853\n",
      "Steps : 840000,  Loss : 0.004556037485599518\n",
      "Steps : 850000,  Loss : 0.0010585456620901823\n",
      "Steps : 860000,  Loss : 0.002800143091008067\n",
      "Steps : 870000,  Loss : 0.0022248467430472374\n",
      "Steps : 880000,  Loss : 0.0007593804621137679\n",
      "Steps : 890000,  Loss : 0.0009051271481439471\n",
      "Steps : 900000,  Loss : 0.004127764143049717\n",
      "Steps : 910000,  Loss : 0.001110098441131413\n",
      "Steps : 920000,  Loss : 0.0009080402669496834\n",
      "Steps : 930000,  Loss : 0.0013641195837408304\n",
      "Steps : 940000,  Loss : 0.0009333558846265078\n",
      "Steps : 950000,  Loss : 0.005681095644831657\n",
      "Steps : 960000,  Loss : 0.001172429183498025\n",
      "Steps : 970000,  Loss : 0.004663550760596991\n",
      "Steps : 980000,  Loss : 0.0009222662774845958\n",
      "Steps : 990000,  Loss : 0.0013204861897975206\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "# construct the replay buffer\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from deep_rl import *\n",
    "\n",
    "replay = UniformReplay(memory_size=int(1e6), batch_size=64)\n",
    "\n",
    "loss_vec = []\n",
    "steps = int(1e6)\n",
    "warm_up = int(1e4)\n",
    "log_interval = int(1e4)\n",
    "\n",
    "\n",
    "net = WNet(11, 1)\n",
    "opt = Adam(list(net.parameters()))\n",
    "\n",
    "for i in range(steps):\n",
    "    if i == 0:\n",
    "        state = env.reset()\n",
    "    \n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    \n",
    "    replay.feed(dict(\n",
    "            state=[state],\n",
    "            action=[action],\n",
    "            reward=[reward],\n",
    "            next_state=[next_state],\n",
    "            mask=1-np.asarray([done], dtype=np.int32),\n",
    "        ))\n",
    "    \n",
    "    state = next_state\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        \n",
    "    if i > warm_up:\n",
    "        transitions = replay.sample()\n",
    "        states = tensor(transitions.state).cuda()\n",
    "        actions = tensor(transitions.action)\n",
    "        rewards = tensor(transitions.reward).unsqueeze(-1).cuda()\n",
    "        next_states = tensor(transitions.next_state)\n",
    "        mask = tensor(transitions.mask).unsqueeze(-1)\n",
    "        \n",
    "        phi_dot_w = net(states.cuda())\n",
    "        loss = (phi_dot_w - rewards).pow(2).mul(0.5).mean()\n",
    "        loss_vec.append(loss)\n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        if i % log_interval == 0:\n",
    "            print(f'Steps : {i},  Loss : {loss}')\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DPC",
   "language": "python",
   "name": "dpc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
