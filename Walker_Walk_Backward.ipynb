{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Walker-Walk-Backward.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM/U6ZlMk8dwJYx8h+bDkcu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IanWangg/DSFPG/blob/master/Walker_Walk_Backward.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEXUCDZxj6Ve",
        "outputId": "cf2a018a-21e9-4820-dd52-5a9554ed6126"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "%cd gdrive/My Drive/Workplace\n",
        "\n",
        "!git clone https://github.com/benelot/pybullet-gym.git\n",
        "\n",
        "%cd pybullet-gym/\n",
        "\n",
        "!pip install -e ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive/Workplace\n",
            "Cloning into 'pybullet-gym'...\n",
            "remote: Enumerating objects: 804, done.\u001b[K\n",
            "remote: Counting objects: 100% (54/54), done.\u001b[K\n",
            "remote: Compressing objects: 100% (48/48), done.\u001b[K\n",
            "remote: Total 804 (delta 21), reused 28 (delta 6), pack-reused 750\u001b[K\n",
            "Receiving objects: 100% (804/804), 19.31 MiB | 13.52 MiB/s, done.\n",
            "Resolving deltas: 100% (437/437), done.\n",
            "Checking out files: 100% (252/252), done.\n",
            "/content/gdrive/My Drive/Workplace/pybullet-gym\n",
            "Obtaining file:///content/gdrive/My%20Drive/Workplace/pybullet-gym\n",
            "Collecting pybullet>=1.7.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/6d/60b97ffc579db665bdd87f2cb47fe1215ae770fbbc1add84ebf36ddca63b/pybullet-3.1.7.tar.gz (79.0MB)\n",
            "\u001b[K     |████████████████████████████████| 79.0MB 37kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pybullet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbK-RNiK4n_v",
        "outputId": "36b20a8f-8d4b-4b74-b89a-b18bf4d85a24"
      },
      "source": [
        "import gym\n",
        "import pybulletgym\n",
        "env = gym.make('Walker2DMuJoCoEnv-v0')\n",
        "env.reset().shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WalkerBase::__init__\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdoR5fugb9gK"
      },
      "source": [
        "# Define the agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39tSaJ0q7LRz",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class ReplayBuffer(object):\n",
        "    def __init__(self, state_dim, action_dim, max_size=int(1e6)):\n",
        "        self.max_size = max_size\n",
        "        self.ptr = 0\n",
        "        self.size = 0\n",
        "\n",
        "        self.start_ptr = 0\n",
        "        self.start_size = 0\n",
        "\n",
        "        self.state = np.zeros((max_size, state_dim))\n",
        "        self.action = np.zeros((max_size, action_dim))\n",
        "        self.next_state = np.zeros((max_size, state_dim))\n",
        "        self.reward = np.zeros((max_size, 1))\n",
        "        self.not_done = np.zeros((max_size, 1))\n",
        "\n",
        "        self.start_state = np.zeros((max_size, state_dim))\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def add(self, state, action, next_state, reward, done):\n",
        "        self.state[self.ptr] = state\n",
        "        self.action[self.ptr] = action\n",
        "        self.next_state[self.ptr] = next_state\n",
        "        self.reward[self.ptr] = reward\n",
        "        self.not_done[self.ptr] = 1. - done\n",
        "\n",
        "        self.ptr = (self.ptr + 1) % self.max_size\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        ind = np.random.randint(self.size, size=batch_size)\n",
        "\n",
        "        return (\n",
        "            torch.FloatTensor(self.state[ind]).to(self.device),\n",
        "            torch.FloatTensor(self.action[ind]).to(self.device),\n",
        "            torch.FloatTensor(self.next_state[ind]).to(self.device),\n",
        "            torch.FloatTensor(self.reward[ind]).to(self.device),\n",
        "            torch.FloatTensor(self.not_done[ind]).to(self.device)\n",
        "        )\n",
        "\n",
        "\n",
        "class Encoder_Decoder(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Encoder_Decoder, self).__init__()\n",
        "\n",
        "        self.e1 = nn.Linear(state_dim + action_dim, 256)\n",
        "        self.e2 = nn.Linear(256, 256)\n",
        "\n",
        "        self.r1 = nn.Linear(256, 1, bias=False)\n",
        "\n",
        "        self.a1 = nn.Linear(256, 256)\n",
        "        self.a2 = nn.Linear(256, action_dim)\n",
        "\n",
        "        self.d1 = nn.Linear(256, 256)\n",
        "        self.d2 = nn.Linear(256, state_dim)\n",
        "\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        l = F.relu(self.e1(torch.cat([state, action], 1)))\n",
        "        l = F.relu(self.e2(l))\n",
        "\n",
        "        r = self.r1(l)\n",
        "\n",
        "        d = F.relu(self.d1(l))\n",
        "        ns = self.d2(d)\n",
        "\n",
        "        d = F.relu(self.a1(l))\n",
        "        a = self.a2(d)\n",
        "\n",
        "        return ns, r, a, l\n",
        "\n",
        "    def latent(self, state, action):\n",
        "        l = F.relu(self.e1(torch.cat([state, action], 1)))\n",
        "        l = F.relu(self.e2(l))\n",
        "        return l\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        self.l1 = nn.Linear(state_dim + action_dim, 256)\n",
        "        self.l2 = nn.Linear(256, 256)\n",
        "        self.l3 = nn.Linear(256, 256)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        q1 = F.relu(self.l1(torch.cat([state, action], 1)))\n",
        "        q1 = F.relu(self.l2(q1))\n",
        "        return self.l3(q1)\n",
        "  \n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        self.l1 = nn.Linear(state_dim, 256)\n",
        "        self.l2 = nn.Linear(256, 256)\n",
        "        self.l3 = nn.Linear(256, action_dim)\n",
        "        \n",
        "        self.max_action = max_action\n",
        "\n",
        "    def forward(self, state):\n",
        "        state = torch.tensor(state).to(device)\n",
        "        a = F.relu(self.l1(state))\n",
        "        a = F.relu(self.l2(a))\n",
        "        return self.max_action * torch.tanh(self.l3(a))\n",
        "\n",
        "\n",
        "class DSFPG(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        state_dim,\n",
        "        action_dim,\n",
        "        max_action,\n",
        "        max_step_before_learning,\n",
        "        buffer_size=int(1e6),\n",
        "        discount=0.99,\n",
        "        tau=0.005\n",
        "    ):\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.encoder_decoder = Encoder_Decoder(state_dim, action_dim).to(device)\n",
        "        self.ed_optimizer = torch.optim.Adam(self.encoder_decoder.parameters(), lr=3e-4)\n",
        "\n",
        "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "        self.actor_target = copy.deepcopy(self.actor)\n",
        "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=3e-4)\n",
        "\n",
        "        self.critic = Critic(state_dim, action_dim).to(device)\n",
        "        self.critic_target = copy.deepcopy(self.critic)\n",
        "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=3e-4)\n",
        "\n",
        "        self.W = torch.ones(1, 256, requires_grad=True, device=device)\n",
        "        self.W_optimizer = torch.optim.Adam([self.W], lr=3e-4)\n",
        "\n",
        "        self.discount = discount\n",
        "        self.tau = tau\n",
        "        self.max_step_before_learning = max_step_before_learning\n",
        "\n",
        "        self.total_it = 0\n",
        "\n",
        "        self.max_action = max_action\n",
        "\n",
        "        self.replay = ReplayBuffer(state_dim, action_dim, buffer_size)\n",
        "\n",
        "\n",
        "    def train_encoder_decoder(self, state, action, next_state, reward, done, batch_size=256):\n",
        "        self.replay.add(state, action, next_state, reward, done)\n",
        "\n",
        "        if self.replay.size > self.max_step_before_learning:\n",
        "            state, action, next_state, reward, not_done = self.replay.sample(batch_size)\n",
        "\n",
        "            recons_next, recons_reward, recons_action, lat = self.encoder_decoder(state, action)\n",
        "            ed_loss = F.mse_loss(recons_next, next_state) + 0.1 * F.mse_loss(recons_reward, reward) + F.mse_loss(recons_action, action)\n",
        "\n",
        "            self.ed_optimizer.zero_grad()\n",
        "            ed_loss.backward()\n",
        "            self.ed_optimizer.step()\n",
        "\n",
        "\n",
        "    def train_SR(self, state, action, next_state, reward, done, batch_size=256):\n",
        "        self.replay.add(state, action, next_state, reward, done)\n",
        "\n",
        "        if self.replay.size > self.max_step_before_learning:\n",
        "            state, action, next_state, reward, not_done = self.replay.sample(batch_size)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                next_action = self.actor_target(next_state)\n",
        "                # add randomness to the next action, this should be removed if the result is not idea \n",
        "                next_action = (next_action + torch.randn_like(next_action) * self.max_action * 0.1).clamp(-self.max_action, self.max_action)\n",
        "\n",
        "                latent = self.encoder_decoder.latent(state, action)\n",
        "                target_Q = latent + self.discount * not_done * self.critic_target(next_state, next_action)\n",
        "\n",
        "            current_Q = self.critic(state, action)\n",
        "            critic_loss = F.mse_loss(current_Q, target_Q)\n",
        "\n",
        "            self.critic_optimizer.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            self.critic_optimizer.step()\n",
        "\n",
        "            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "\n",
        "    def train_w(self, state, action, next_state, reward, done, batch_size=256):\n",
        "        self.replay.add(state, action, next_state, reward, done)\n",
        "\n",
        "        if self.replay.size > self.max_step_before_learning:\n",
        "            state, action, next_state, reward, not_done = self.replay.sample(batch_size)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                latent = self.encoder_decoder(state, action)\n",
        "            \n",
        "            reward_estimate = latent * self.W\n",
        "            W_loss = F.mse_loss(reward_estimate, reward)\n",
        "\n",
        "            self.W_optimizer.zero_grad()\n",
        "            W_loss.backward()\n",
        "            self.W_optimizer.step()\n",
        "\n",
        "    def train_actor(self, state, action, next_state, reward, done, batch_size=256):\n",
        "        self.replay.add(state, action, next_state, reward, done)\n",
        "\n",
        "        if self.replay.size > self.max_step_before_learning:\n",
        "            state, action, next_state, reward, not_done = self.replay.sample(batch_size)\n",
        "\n",
        "            actor_loss = -(self.critic(state, self.actor(state)) * self.W).mean()\n",
        "\n",
        "            self.actor_optimizer.zero_grad()\n",
        "            actor_loss.backward()\n",
        "            self.actor_optimizer.step()\n",
        "\n",
        "    def select_action(self, state):\n",
        "        with torch.no_grad():\n",
        "            action = self.actor(state)\n",
        "        return action\n",
        "\n",
        "    def train(self, state, action, next_state, reward, done):\n",
        "        self.train_encoder_decoder(state, action, next_state, reward, done)\n",
        "        self.train_SR(state, action, next_state, reward, done)\n",
        "        self.train_w(state, action, next_state, reward, done)\n",
        "        self.train_actor(state, action, next_state, reward, done)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxxdT3WW7MBB"
      },
      "source": [
        "# Use PyBullet Built-in Locomotion Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92SSpcannBzw"
      },
      "source": [
        "from tqdm import trange\n",
        "import gym\n",
        "import pybulletgym\n",
        "\n",
        "def train_agent(agent_func, \n",
        "                env_name, # this should be an env object\n",
        "                runs=1,\n",
        "                max_steps=int(1e6),\n",
        "                max_step_before_learning=int(3e4)\n",
        "                ):\n",
        "    returns_timing = []\n",
        "    returns_value = []\n",
        "    agents = []\n",
        "    env = gym.make(env_name)\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "    max_action = float(env.action_space.high[0])\n",
        "    \n",
        "    for run in trange(runs, desc='runs'):\n",
        "        env.seed(run + 100)\n",
        "        total_steps = 0\n",
        "        done = True\n",
        "        # each element in returns array should be of shape [episodic_return, steps]\n",
        "        # if an episode is not over, episodic_return is 0\n",
        "        rewards = []\n",
        "        episodic_return = 0\n",
        "        agent = agent_func(state_dim=state_dim,\n",
        "                            action_dim=action_dim,\n",
        "                            max_action=max_action,\n",
        "                            max_step_before_learning=max_step_before_learning,\n",
        "                            buffer_size=max_steps)\n",
        "\n",
        "        while total_steps < max_steps:\n",
        "            if done:\n",
        "                state = env.reset()\n",
        "                rewards.append([total_steps, episodic_return])\n",
        "                episodic_return = 0\n",
        "\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            agent.train(state, action, next_state, reward, done)\n",
        "            episodic_return += reward\n",
        "        \n",
        "        returns_timing.append(rewards[:, 0])\n",
        "        returns_timing.append(rewards[:, 1])\n",
        "        agents.append(agents)\n",
        "\n",
        "        filename = f'./state_dict/{agent_func.__name__}-{env_name}-{random_seed}.pt'\n",
        "        torch.save(agent.state_dict(), filename)\n",
        "    \n",
        "    return agents, returns_timing, returns_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "id": "XteuKDI3xCFm",
        "outputId": "a8bb4385-52d9-4b3c-d77c-2e2e78a4b8f4"
      },
      "source": [
        "agents, t, r = train_agent(agent_func=DSFPG,\n",
        "                           env_name='Walker2DMuJoCoEnv-v0')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
            "\n",
            "\n",
            "\n",
            "runs:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WalkerBase::__init__\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:111: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-648aad986336>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m agents, t, r = train_agent(agent_func=DSFPG,\n\u001b[0;32m----> 2\u001b[0;31m                            env_name='Walker2DMuJoCoEnv-v0')\n\u001b[0m",
            "\u001b[0;32m<ipython-input-29-045353aa20ab>\u001b[0m in \u001b[0;36mtrain_agent\u001b[0;34m(agent_func, env_name, runs, max_steps, max_step_before_learning)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mepisodic_return\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-f1defa2ff994>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, state, action, next_state, reward, done)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_encoder_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_SR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_actor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-f1defa2ff994>\u001b[0m in \u001b[0;36mtrain_w\u001b[0;34m(self, state, action, next_state, reward, done, batch_size)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_step_before_learning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnot_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'replay_buffer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcDhjmbUvHeo"
      },
      "source": [
        "}def plot_rewards(rewards, plot_seperate=True , clip=int(1e6), title='unnamed'):\n",
        "    smooth = 5000\n",
        "    \n",
        "    colors = ['red', 'blue', 'green', 'm', 'k', 'y', '#999999']\n",
        "    \n",
        "    plt.figure(figsize=(16,6), dpi=200)\n",
        "    if(plot_seperate):\n",
        "        for k, v in rewards.items():\n",
        "            for t, r in zip(v[0], v[1]):\n",
        "                plt.plot(t, r, label=k)\n",
        "        plt.legend(), plt.show()\n",
        "        return\n",
        "    \n",
        "    for j, (k, v) in enumerate(rewards.items()):\n",
        "        r_vec = np.zeros((len(v[0]), clip-smooth+1))\n",
        "        for i, (t, r) in enumerate(zip(v[0], v[1])):\n",
        "            r_vec[i,:] = convolve(np.interp(np.arange(clip), t, r), smooth)\n",
        "    \n",
        "        mean = np.mean(np.array(r_vec), axis=0)\n",
        "        std = np.std(np.array(r_vec), axis=0)\n",
        "        plt.plot(mean, label=k, color=colors[j])\n",
        "        plt.fill_between(np.arange(0, len(mean)), mean+std, mean-std, facecolor=colors[j], alpha=0.3)\n",
        "    \n",
        "    plt.xlabel('timesteps'), plt.ylabel('episodic returns')\n",
        "    plt.title(title)\n",
        "    plt.legend(loc='lower right'), plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}